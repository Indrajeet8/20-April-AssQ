{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18650344-dc57-4181-ac63-398b93733751",
   "metadata": {},
   "source": [
    "### KNN 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f4385-2d2e-4bc2-a6fe-8631ec4d8549",
   "metadata": {},
   "source": [
    "Assignment Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401b123a-569c-46ea-8cd7-af3f0c0707a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a28b6-40c0-4341-b59a-1e83db1976d1",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors Algorithm. The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bcec237-2fbf-409b-b711-a0b85edf9dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4778f4-7342-484b-adda-e71b9c718aa1",
   "metadata": {},
   "source": [
    "Square Root Method: Take square root of the number of samples in the training dataset.\n",
    "Cross Validation Method: We should also use cross validation to find out the optimal value of K in KNN. ...\n",
    "Domain Knowledge also plays a vital role while choosing the optimum value of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b6917-04b6-417a-b1d0-69fc1ca5f866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd86366-ceed-44bb-9fce-adb604e1cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf0dcb-ba08-4f8c-821b-8d15b4e66b44",
   "metadata": {},
   "source": [
    "Carefully explain the differences between the KNN classifier and KNN regression methods. So the main difference is the fact that for the classifier approach, the algorithm assumes the outcome as the class of more presence, and on the regression approach the response is the average value of the nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e08c0c6-0553-4a0e-b3c6-db2b12302355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4305707-94b6-4d41-b9e8-30ec133f7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6501b1f-ad2f-4941-8704-2091b48ebb81",
   "metadata": {},
   "source": [
    "The test data is used to evaluate the performance of the model. The model is tested on the test data by using it to make predictions and comparing these predictions to the actual target values. When training a kNN classifier, it's essential to normalize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5becd3-4e0b-4063-b18e-723e4942ad72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe56df4-e4cd-4532-bf3e-4615e496e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa40cae9-884b-463c-a5eb-a83ba1fdabdb",
   "metadata": {},
   "source": [
    "The “Curse of Dimensionality” is a tongue in cheek way of stating that there's a ton of space in high-dimensional data sets. The size of the data space grows exponentially with the number of dimensions. This means that the size of your data set must also grow exponentially in order to keep the same density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb979968-0220-4cc5-ac46-80b466a3b3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "917784b9-2c97-43cf-a22f-87a569b568f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d982f0d5-e2e5-4718-88d8-2dfe66d4884d",
   "metadata": {},
   "source": [
    "The idea in kNN methods is to identify 'k' samples in the dataset that are similar or close in the space. Then we use these 'k' samples to estimate the value of the missing data points. Each sample's missing values are imputed using the mean value of the 'k'-neighbors found in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e6a867-d0c6-44e4-b170-8f5889a73ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ff81efd-41b3-4456-8b35-874b726e8191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74049a04-45af-4745-97b3-1cf829b9953b",
   "metadata": {},
   "source": [
    "Certainly! The K-Nearest Neighbors (KNN) algorithm can be used for both classification and regression tasks. Here's a comparison between KNN classifier and regressor in terms of performance and suitability for different problem types:\n",
    "\n",
    "1. **KNN Classifier**:\n",
    "   - **Objective**: It's used for classification tasks where the output variable is a categorical or discrete class label.\n",
    "   - **Performance**:\n",
    "     - **Pros**: Simple to implement, works well with small to medium-sized datasets, and is robust to noisy data.\n",
    "     - **Cons**: Computationally expensive during prediction for large datasets as it requires calculating distances to all training samples.\n",
    "   - **Suitability**:\n",
    "     - Works best with labeled data where classes are well-separated.\n",
    "     - Suitable for problems like image recognition, text categorization, and recommendation systems where the classes have clear boundaries.\n",
    "\n",
    "2. **KNN Regressor**:\n",
    "   - **Objective**: It's used for regression tasks where the output variable is continuous or numerical.\n",
    "   - **Performance**:\n",
    "     - **Pros**: Easy to understand and implement, doesn’t make assumptions about data distribution.\n",
    "     - **Cons**: Sensitive to outliers, and, similar to the classifier, computationally expensive for large datasets during prediction.\n",
    "   - **Suitability**:\n",
    "     - Appropriate when the relationships between features and target variables are nonlinear or when the data doesn't follow a specific parametric distribution.\n",
    "     - Useful in scenarios like predicting housing prices, stock prices, or any continuous value prediction.\n",
    "\n",
    "**Which one is better for which type of problem?**\n",
    "- **Use KNN Classifier**:\n",
    "  - When dealing with classification tasks where the output is categorical, e.g., classifying emails as spam or not spam, identifying handwritten digits, etc.\n",
    "  - Suitable for datasets with discrete class labels and relatively smaller sizes.\n",
    "\n",
    "- **Use KNN Regressor**:\n",
    "  - For regression tasks involving predicting continuous values, such as predicting house prices, sales forecasting, or any numerical prediction where data doesn't follow a linear pattern.\n",
    "  - Effective when dealing with datasets where relationships between variables are nonlinear or not explicitly defined.\n",
    "\n",
    "In summary, the choice between KNN classifier and regressor depends on the nature of the problem, the type of output variable, dataset size, and the characteristics of the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8876ed86-65c1-4220-aa0d-56b46bbbac2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa2a09ac-87fc-4400-b5a3-a9c0918b2faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4bf4ac-7426-48dc-bc35-25e28cbf4b0b",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a simple yet effective method used for both classification and regression tasks. However, it has its strengths and weaknesses:\n",
    "\n",
    "### Strengths of KNN:\n",
    "\n",
    "1. **Easy Implementation:** KNN is easy to understand and implement, making it a good starting point for beginners in machine learning.\n",
    "  \n",
    "2. **Non-Parametric:** It doesn't make any assumptions about the underlying data distribution, which can be beneficial when dealing with non-linear data.\n",
    "\n",
    "3. **Versatile:** It can work well with both classification and regression problems.\n",
    "\n",
    "### Weaknesses of KNN:\n",
    "\n",
    "1. **Computational Complexity:** As the dataset grows, the computation cost of KNN increases significantly because it needs to compute distances between the test sample and all training samples.\n",
    "\n",
    "2. **Sensitivity to High-Dimensional Data:** In high-dimensional spaces, the notion of distance between points may lose its meaning (curse of dimensionality), impacting KNN's performance negatively.\n",
    "\n",
    "3. **Need for Optimal K:** The value of 'K' (the number of neighbors) needs to be chosen carefully. Too small K can make the model sensitive to noise, while a larger K may lead to oversmoothing.\n",
    "\n",
    "### Addressing KNN's Weaknesses:\n",
    "\n",
    "1. **Dimensionality Reduction:** Preprocess data to reduce dimensionality using techniques like Principal Component Analysis (PCA) or feature selection to mitigate the curse of dimensionality.\n",
    "\n",
    "2. **Normalization/Standardization:** Scaling the features to a similar range can significantly improve the performance of KNN.\n",
    "\n",
    "3. **Use of Distance Metrics:** Experiment with different distance metrics (Euclidean, Manhattan, etc.) to find the one that fits the data better.\n",
    "\n",
    "4. **Cross-Validation for Optimal K:** Perform cross-validation to find the optimal value of 'K' that gives the best performance on the validation set.\n",
    "\n",
    "5. **Use of Approximate Nearest Neighbors (ANN):** For larger datasets, employing techniques like KD-trees, Ball-trees, or other approximate nearest neighbor algorithms can significantly reduce computation time.\n",
    "\n",
    "6. **Ensemble Methods:** Combine predictions from multiple KNN models or ensemble methods like Bagging or Boosting to improve overall performance and reduce variance.\n",
    "\n",
    "KNN is a versatile algorithm, but its performance depends heavily on the nature and size of the dataset. Addressing its weaknesses through proper preprocessing, parameter tuning, and considering alternative techniques can enhance its effectiveness in classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e21d3-4101-4d3b-a2f0-84078d288a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e04047-4d49-4883-8e3f-6aca4fe46626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e962fb-1869-428e-8fe3-1f5e77413d82",
   "metadata": {},
   "source": [
    "Euclidean distance and Manhattan distance are two common distance metrics used in machine learning, including the K-Nearest Neighbors (KNN) algorithm. These metrics measure the distance between two points in a multidimensional space but differ in their calculation methods:\n",
    "\n",
    "### Euclidean Distance:\n",
    "- **Formula:** \\( \\text{Euclidean Distance} = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} \\)\n",
    "- **Calculation:** It measures the straight-line distance between two points in Euclidean space.\n",
    "- **Properties:**\n",
    "  - Represents the shortest path between two points.\n",
    "  - Captures the \"as-the-crow-flies\" distance, which is the direct distance between two points.\n",
    "  - Works well when dealing with continuous features and assumes a continuous distribution of data.\n",
    "- **Usage:** Commonly used in applications where spatial relationships or geometric properties are important, such as image recognition, computer vision, and clustering algorithms.\n",
    "\n",
    "### Manhattan Distance (Taxicab or City Block Distance):\n",
    "- **Formula:** \\( \\text{Manhattan Distance} = \\sum_{i=1}^{n} |x_i - y_i| \\)\n",
    "- **Calculation:** It measures the distance between two points by summing the absolute differences between their coordinates along each dimension.\n",
    "- **Properties:**\n",
    "  - Represents the distance between two points as the distance a taxi would travel along streets to reach one point from the other, only moving parallel to the axes (horizontally and vertically).\n",
    "  - Ignores diagonal movements and only considers movements along the grid lines.\n",
    "  - Suitable for cases where movement is restricted to a grid-like structure or when dealing with categorical variables.\n",
    "- **Usage:** Commonly used in circuit design, game development (pathfinding algorithms), and situations where movement occurs on a grid-like structure.\n",
    "\n",
    "### Differences between Euclidean and Manhattan Distance:\n",
    "\n",
    "1. **Calculation Method:**\n",
    "   - Euclidean distance computes the straight-line or direct distance between two points.\n",
    "   - Manhattan distance calculates the distance by summing the absolute differences between coordinates along each dimension.\n",
    "\n",
    "2. **Movement Consideration:**\n",
    "   - Euclidean distance considers direct or diagonal movements between points.\n",
    "   - Manhattan distance only considers movements along the axes, akin to moving on a grid-like structure.\n",
    "\n",
    "3. **Sensitivity to Dimensionality:**\n",
    "   - Euclidean distance is sensitive to all dimensions and is influenced by differences in all directions.\n",
    "   - Manhattan distance treats each dimension separately and is less affected by outliers or differences along a single dimension.\n",
    "\n",
    "Both distance metrics have their advantages and are suitable for different scenarios. The choice between Euclidean and Manhattan distance (or other distance metrics) in KNN often depends on the nature of the data, the underlying problem, and the dimensionality of the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd9de544-6a5c-4b62-89ef-f67949218a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42510cc9-6eda-4331-9ffb-2c999e0a0958",
   "metadata": {},
   "source": [
    "Feature scaling is essential in the K-Nearest Neighbors (KNN) algorithm for the following reasons:\n",
    "\n",
    "1. **Distance Computation:** KNN relies heavily on computing distances between data points to identify nearest neighbors. Features with larger scales or ranges can dominate the distance calculation. Scaling helps in normalizing the range of all features, ensuring that no single feature disproportionately influences the distance metric.\n",
    "\n",
    "2. **Equalizing Feature Contributions:** Without scaling, features with larger numerical ranges or magnitudes might overshadow or dominate the contributions of features with smaller ranges. Scaling ensures that all features contribute equally to the similarity measures used in KNN.\n",
    "\n",
    "3. **Improving Model Performance:** Scaling features can lead to better performance in KNN. It prevents biases towards features with larger scales, allowing the algorithm to identify similarities based on the actual relationships in the data, rather than the magnitude of the features.\n",
    "\n",
    "4. **Handling Diverse Feature Scales:** In datasets where features have different units or scales, feature scaling makes the algorithm more robust and prevents inaccurate results due to the arbitrary scales of features.\n",
    "\n",
    "### Impact of Feature Scaling on KNN:\n",
    "\n",
    "- **Consistent Distance Metrics:** Scaling ensures that distance metrics (such as Euclidean or Manhattan distances) are calculated consistently across all features, making comparisons more meaningful.\n",
    "  \n",
    "- **Avoiding Misinterpretation of Feature Importance:** Without scaling, features with larger scales might erroneously be considered more important by the algorithm. Scaling prevents this misinterpretation.\n",
    "\n",
    "- **Convergence and Computational Efficiency:** Properly scaled features can lead to faster convergence during the algorithm's computation process, improving computational efficiency.\n",
    "\n",
    "### Common Scaling Techniques in KNN:\n",
    "\n",
    "- **Min-Max Scaling (Normalization)**\n",
    "- **Standardization (Z-score Normalization)**\n",
    "- **Robust Scaling**\n",
    "  \n",
    "Choosing the appropriate scaling method depends on the nature of the data, the range of values in the features, and the specific requirements of the KNN algorithm applied to the dataset. Overall, feature scaling ensures that KNN operates effectively by removing biases caused by differences in feature scales, thus leading to more accurate and reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91690417-719f-4039-812c-787a1790c4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
